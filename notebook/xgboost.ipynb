{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1a75e52-dca4-42b4-9c53-04f033b4db2b",
   "metadata": {},
   "source": [
    "# **XGBoost and Its Mathematical Intuition**\n",
    "XGBoost (Extreme Gradient Boosting) is a highly efficient, scalable, and optimized implementation of gradient boosting for decision trees. It builds a series of decision trees sequentially, where each tree corrects the errors made by the previous trees. The model aggregates these trees to make final predictions. XGBoost stands out due to its regularization techniques, parallel processing, and ability to handle sparse data.\n",
    "---\n",
    "## **Mathematical Intuition**\n",
    "Let $(x_i, y_i)$ be the training data with $n$ samples. XGBoost minimizes a loss function with regularization to avoid overfitting.\n",
    "### **Objective Function**  \n",
    "The objective is formulated as:\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\sum_{i=1}^{n} l(y_i, \\hat{y}_i^{(t)}) + \\sum_{k=1}^{t} \\Omega(f_k)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $l(y_i, \\hat{y}_i^{(t)})$ is the loss function, often Mean Squared Error for regression or Log Loss for classification.  \n",
    "- $\\Omega(f_k) = \\gamma T + \\frac{1}{2} \\lambda \\sum_j w_j^2$  \n",
    "  is the regularization term to penalize model complexity.  \n",
    "- $f_k$ represents a decision tree.\n",
    "\n",
    "At each boosting iteration, XGBoost adds a new function $f_t(x)$ that minimizes the following Taylor expansion of the loss:\n",
    "\n",
    "$$\n",
    "L^{(t)} \\approx \\sum_{i=1}^{n} \\left[g_i f_t(x_i) + \\frac{1}{2} h_i f_t^2(x_i) \\right] + \\Omega(f_t)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $g_i = \\frac{\\partial l(y_i, \\hat{y}_i)}{\\partial \\hat{y}_i}$ (first-order gradient)  \n",
    "- $h_i = \\frac{\\partial^2 l(y_i, \\hat{y}_i)}{\\partial \\hat{y}_i^2}$ (second-order gradient)\n",
    "\n",
    "The algorithm splits nodes in decision trees based on maximizing the gain:\n",
    "\n",
    "$$\n",
    "Gain = \\frac{1}{2} \\left[\\frac{(G_L + G_R)^2}{H_L + H_R + \\lambda} - \\frac{G_L^2}{H_L + \\lambda} - \\frac{G_R^2}{H_R + \\lambda}\\right] - \\gamma\n",
    "$$\n",
    "\n",
    "Where $G$ and $H$ are the sums of gradients and Hessians for left and right branches.\n",
    "---\n",
    "## **Example: Regression Problem**\n",
    "Suppose you want to predict house prices based on features like size and location. In each boosting round:\n",
    "1. The first decision tree predicts a baseline value (e.g., average price).\n",
    "2. Calculate residuals between actual prices and predicted prices.\n",
    "3. A new tree is fit to predict the gradients (residuals) from the previous step.\n",
    "4. Update predictions by combining previous and current predictions.\n",
    "---\n",
    "## **Differences from Gradient Boosting**\n",
    "| **Aspect**        | **Gradient Boosting (GBM)** | **XGBoost**                    |\n",
    "|------------------|-----------------------------|---------------------------------|\n",
    "| Speed            | Moderate                    | Faster (parallelization)       |\n",
    "| Regularization   | No built-in regularization   | L1 and L2 regularization       |\n",
    "| Tree Pruning     | No pruning                   | Prunes based on max depth      |\n",
    "| Handling Missing | Basic                        | Optimized                      |\n",
    "| Objective        | Gradient updates only        | Uses second-order derivatives  |\n",
    "---\n",
    "In summary, XGBoost improves upon traditional gradient boosting with regularization, second-order optimization, and efficient computation techniques, making it more powerful and scalable for large datasets.\n",
    "\n",
    "Key changes made:\n",
    "1. Replaced \\( \\) with $ $ for inline math\n",
    "2. Added blank lines before and after display math equations ($$)\n",
    "3. Ensured proper spacing around equations\n",
    "4. Fixed the LaTeX formatting for subscripts and superscripts\n",
    "5. Maintained consistent math notation throughout the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6828a0-1049-4072-a3bc-2eb8ed96b776",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
